,L1_attention-mean_word-mean_norm,L2_attention-mean_word-mean_norm,L3_attention-mean_word-mean_norm,L4_attention-mean_word-mean_norm,L5_attention-mean_word-mean_norm,L6_attention-mean_word-mean_norm,L7_attention-mean_word-mean_norm,L8_attention-mean_word-mean_norm,L9_attention-mean_word-mean_norm,L10_attention-mean_word-mean_norm,L11_attention-mean_word-mean_norm,L12_attention-mean_word-mean_norm,L13_attention-mean_word-mean_norm,L14_attention-mean_word-mean_norm,L15_attention-mean_word-mean_norm,L16_attention-mean_word-mean_norm,L17_attention-mean_word-mean_norm,L18_attention-mean_word-mean_norm,L19_attention-mean_word-mean_norm,L20_attention-mean_word-mean_norm,L21_attention-mean_word-mean_norm,L22_attention-mean_word-mean_norm,L23_attention-mean_word-mean_norm,L24_attention-mean_word-mean_norm,L25_attention-mean_word-mean_norm,L26_attention-mean_word-mean_norm,L27_attention-mean_word-mean_norm,L28_attention-mean_word-mean_norm,L29_attention-mean_word-mean_norm,L30_attention-mean_word-mean_norm,L31_attention-mean_word-mean_norm,L32_attention-mean_word-mean_norm,L33_attention-mean_word-mean_norm,L34_attention-mean_word-mean_norm,L35_attention-mean_word-mean_norm,L36_attention-mean_word-mean_norm,L37_attention-mean_word-mean_norm,L38_attention-mean_word-mean_norm,L39_attention-mean_word-mean_norm,L40_attention-mean_word-mean_norm,L41_attention-mean_word-mean_norm,L42_attention-mean_word-mean_norm,L43_attention-mean_word-mean_norm,L44_attention-mean_word-mean_norm,L45_attention-mean_word-mean_norm,L46_attention-mean_word-mean_norm,L47_attention-mean_word-mean_norm,L48_attention-mean_word-mean_norm
study_1_microsoft-deberta-base-True,0.43670815798700763,0.3768342202920161,0.37393645626690425,0.3748811916626856,0.3906514247745464,0.384159074159951,0.4167115706755781,0.4055235722482351,0.4088856946435596,0.43249973151205384,0.4034726307243918,0.41133753907378906,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_bert-base-uncased-True,0.6318247681003762,0.3463640495113156,0.41964511242766034,0.43274501845922403,0.5074415969225397,0.4617934677781757,0.4731135787895442,0.3750533895190065,0.4482996666974277,0.5237088081120732,0.4686779708619518,0.41098196332973147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_microsoft-deberta-xlarge-True,0.43363469325176296,0.4017457870811517,0.3992545057517271,0.4280249542493518,0.4116206300484322,0.3831573910070489,0.390888357386499,0.3795587659703256,0.3741398511756599,0.36800125740097356,0.3827516879912992,0.35995718974020774,0.3807683427524635,0.37202468187897625,0.3913318863451022,0.38224884930399977,0.36108253733833373,0.38214424688666127,0.3635366724878756,0.3549482293463294,0.3836121606244159,0.3429656584394165,0.31801473009830655,0.2724060589625729,0.3239786886708419,0.3513529597179089,0.35020732979451624,0.3668197327167576,0.37253401680710013,0.35212760203857046,0.362196238876888,0.3628456672203489,0.3368001673448672,0.34301020256979936,0.3738117394175629,0.3453490605663868,0.36833038962251835,0.35876457836763587,0.37914035630097154,0.3737892040465942,0.35562098093061184,0.3804248159357688,0.3602600053068849,0.3381880688860034,0.37365561833796307,0.37998625416080145,0.3829922503737175,0.4220769373469685
study_1_microsoft-deberta-v2-xlarge-True,0.5942498536794736,0.3949320523382551,0.46516067515205484,0.5006024616812876,0.5036357823622439,0.4767361411299457,0.43975321536704104,0.40971771312560384,0.4458569163739402,0.4323517688889587,0.48165183319506993,0.3657097166960885,0.2703882725650074,0.33600131887096624,0.40045197865597226,0.42856629555833003,0.3315118108970481,0.4053212288659312,0.42913859138740085,0.4343375649390811,0.4398025525031663,0.33358400884146877,0.314456240828024,0.3872509815193591,,,,,,,,,,,,,,,,,,,,,,,,
study_1_bert-base-multilingual-cased-True,0.45907804339518,0.28071565720828867,0.3915793021338562,0.4616629118144242,0.4985214921872422,0.46474562271125097,0.45749172063718163,0.4386372950497603,0.44386689196210277,0.45744271342332526,0.5104424785746713,0.4168426031747468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_google-reformer-crime-and-punishment-True,0.1837849355723856,0.21955182700304549,0.12180771587302207,0.24463431159555793,0.11600754191067013,0.2405722144971238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_bert-large-uncased-True,0.621135090822037,0.26106141482098144,0.039007906352214045,0.08120080802662946,0.11533406561792597,0.1854270519085162,0.15470467999072646,0.25415310644545613,0.22824569706306921,0.254380826587058,0.3014708019972817,0.3723112582326897,0.4149142588876572,0.5008087142312915,0.4699369649538864,0.40072214238441484,0.4740021629411775,0.4546581663074657,0.4319332224374091,0.44771105225552144,0.4269666877679755,0.3285103390371082,0.29195573565128907,0.32365064975536895,,,,,,,,,,,,,,,,,,,,,,,,
study_1_xlm-roberta-large-True,0.3815609117309136,0.36568481937380304,0.3446143964818005,0.1256882241229224,0.17903783081333308,0.2694423880980544,0.22523544074879584,0.21678140076770497,0.4087193435040634,0.32891460801115885,0.4574332080097014,0.2920187305954117,0.42490266897400586,0.37207352988079573,0.3809262338096399,0.32329834195603646,0.4041588206748071,0.3795536790117971,0.3479976626699461,0.36605397317463145,0.4105909918057754,0.27355961665330836,0.4621344435388937,0.35222999051326165,,,,,,,,,,,,,,,,,,,,,,,,
study_1_albert-xlarge-v2-True,0.5414543089517424,0.43048735793744924,0.49829684994040246,0.44185286430515486,0.44921666482620354,0.45051553471143724,0.4499119001625704,0.4338320293072459,0.4372457323464906,0.4230995458927975,0.4170673216324132,0.43296354582203106,0.4164124103453582,0.4165153414670771,0.4188611766979402,0.421028015097467,0.41298184401717775,0.41203249183586643,0.42098245271037243,0.410508628354838,0.4310412287643492,0.3990695673390355,0.4093983471517643,0.2779718670581942,,,,,,,,,,,,,,,,,,,,,,,,
study_1_roberta-base-True,0.46313795653457485,0.42519906328302265,0.44337878913505885,0.40045920298232296,0.39605661409014303,0.3839768690190783,0.3750400465360161,0.3935810598550894,0.37190021727902645,0.3604431809527218,0.3425829314120792,0.39707948291597056,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_gpt2-True,0.4116727650786554,0.3864093899560664,0.4043853358811527,0.38768414799939166,0.405536398391376,0.35012762675781145,0.36064733613661326,0.34427740707220295,0.3380569650429548,0.3418327815439545,0.34684564022519504,0.32952559550693344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_microsoft-deberta-v2-xxlarge-True,0.6074175634852804,0.4243767835905263,0.47084724873838885,0.5071108964752349,0.5190306332972878,0.4872140024679306,0.46494673042173873,0.44917911942850375,0.4003952865663557,0.3964349902254501,0.45727060332369146,0.40418879173896843,0.33910628941632176,0.36611883759711683,0.3839245471812596,0.44039805075495453,0.3918532732383288,0.4476664798787189,0.38822472116522033,0.43930703676890676,0.43012812371834785,0.3207517323457455,0.30146989671979374,0.3534954374465053,0.2885972102245293,0.3380529620493141,0.333774675425211,0.43327283954900514,0.302763333479189,0.3960078791865848,0.3953806728481503,0.4419647895218518,0.4570432006866201,0.36565628608897655,0.29617004170370875,0.3323443981089382,0.36613610701914834,0.25797664306652734,0.3253274961229203,0.39589151621101315,0.267122423886269,0.37823851754057064,0.4078461544263975,0.39855637580616354,0.45133377797440444,0.33281289625292815,0.3142238630605298,0.3670808405321784
study_1_xlnet-large-cased-True,0.3392950879826115,0.3574304274102089,0.34775867939600436,0.3971860579056533,0.3576328784278245,0.3548299033879732,0.37780716864407266,0.4160350528020869,0.3654788816499939,0.39419350587774543,0.39286721494748217,0.39416875123771067,0.3890336930037154,0.3897943233165337,0.40483069264482563,0.38789081742834275,0.3784517269459772,0.3839580139605622,0.3399703685001164,0.2921269545955689,0.28015065400053113,0.24556640411127853,0.19009002521996432,0.42768196898662003,,,,,,,,,,,,,,,,,,,,,,,,
study_1_google-bigbird-roberta-base-True,0.4952042513583928,0.5479411135060653,0.5477751061162107,0.4665834621371379,0.491580522724509,0.47677008197467835,0.4444632063758435,0.4632044493896694,0.4387788375468128,0.4300330272157067,0.42139276936657655,0.3899167819408175,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_google-rembert-True,0.40049794401085664,0.25778163441354274,0.36208091797260467,0.18073410655896643,0.1736005971383672,0.0844804601174366,0.11868208028121044,0.19774171274765667,0.22778503597300764,0.16732627396909328,0.27697158948765,0.33401631292989403,0.3788298916422535,0.4385946413353046,0.4049698825891377,0.4353644437404263,0.4359492220750645,0.503875475638648,0.45047916899889856,0.42956576816723485,0.4616241594612353,0.4072520837275711,0.3926327758353905,0.45409070560490283,0.3180237889344222,0.2613633192630697,0.1380424303461227,0.06559524170374478,0.05450718949442957,0.10941677531924113,0.0583765417270675,0.06416230699142468,,,,,,,,,,,,,,,,
study_1_bert-large-cased-True,0.5833414228406979,0.36383567006644296,0.15658987117548293,0.19455364561781985,0.19820584301298688,0.18551416331830664,0.2437207325011645,0.2549765369143416,0.3257839941052654,0.2801131086028313,0.32621106058636307,0.37711216583594714,0.3402996353073326,0.3348931603707275,0.4383202602019751,0.377824735848344,0.40036883605281276,0.44052294057765623,0.3891465568553548,0.4798269832106647,0.4762511217935424,0.42841233507925347,0.36944956141168367,0.40958554577766315,,,,,,,,,,,,,,,,,,,,,,,,
study_1_facebook-bart-base-True,0.4528407186851502,0.43549577779685905,0.4078352249841439,0.41620397282849153,0.4055367817626396,0.42557517122080596,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_gpt2-medium-True,0.32076927907361075,0.34033215197686145,0.3709853135197024,0.40770421426838727,0.4078136019218373,0.3934762295452522,0.3997158843168858,0.39056649360333534,0.397291908779723,0.3367059872756475,0.35906111454358824,0.35620726607562175,0.34630863136587603,0.3446302500393373,0.33785706461378656,0.34598768960054177,0.3472762047723591,0.3517627137474982,0.3633615744812305,0.32514020974043534,0.3306587691958286,0.35443099084868535,0.34477502875049476,0.33744804926489036,,,,,,,,,,,,,,,,,,,,,,,,
study_1_google-bigbird-roberta-large-True,0.5272258744590349,0.26252293516741804,0.44268896525008117,0.5507242155380382,0.49248982769903243,0.48028303999806415,0.4113206958625052,0.36334214920535685,0.3375323982755112,0.3962948168449428,0.4001107998287063,0.44273217338003984,0.48331181481277496,0.4124970447912772,0.3946056642550144,0.4096599937724798,0.394860956889359,0.37949741302105217,0.36933711464996805,0.4192314907051242,0.3754001120143322,0.389085391992382,0.3742601877357128,0.2618794823630622,,,,,,,,,,,,,,,,,,,,,,,,
study_1_mwesner-reformer-clm-True,0.21450537623361227,0.22286089668817533,0.20400537253445578,0.23313648884345228,-0.008392222290695599,0.23566683896833843,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_bert-base-cased-True,0.5836995723338151,0.2531089535958631,0.39374871295314107,0.4030868584214988,0.4331772363539347,0.4506098430529087,0.45485240749551153,0.3725404095795295,0.4397630382851955,0.48406383549353976,0.4827753417245381,0.49352458275540223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_allenai-longformer-base-4096-True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_facebook-bart-large-True,0.44992937823742796,0.40184354846223136,0.3839722654676822,0.3931845762270049,0.4349307629105045,0.4129915538951296,0.41834405011460934,0.4050955648171286,0.39283699622122303,0.370298280491163,0.3744164115118344,0.3697112289996823,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_distilbert-base-cased-True,0.5232581367274386,0.42489177326063565,0.5180483743523336,0.5056578985931971,0.513679530274253,0.5036257300811542,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_facebook-muppet-roberta-large-True,0.45523859566844044,0.35943552489458375,0.37958828406308814,0.4515074242233389,0.4056259568365076,0.3536032314955393,0.3645388896232558,0.3452811751773688,0.36806998143731945,0.3419140757705101,0.37667878225135315,0.37015039437140795,0.35368244271844035,0.31824265871736673,0.3206335652637361,0.36098061957523403,0.35121586381157555,0.3531775467244148,0.3717585989014586,0.33366464680232244,0.30212355241209643,0.3232854582033253,0.32556310722985604,0.3181951883302434,,,,,,,,,,,,,,,,,,,,,,,,
study_1_xlnet-base-cased-True,0.39810645995569915,0.43650812111213233,0.37504505770341706,0.351785845502427,0.38041580924109647,0.40238961555073494,0.38869746763670104,0.41958940596560473,0.3810508604249816,0.36317712159942145,0.3943152379425823,0.33415820555847636,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_squeezebert-squeezebert-uncased-True,0.45793053965560854,0.43755071127455003,0.4473970198221267,0.40868989763719676,0.3687516411654568,0.4246415054276681,0.277916112386095,0.3659785132558665,0.3622582113823957,0.3768501598312693,0.4218221586952361,0.4506918249627423,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_facebook-muppet-roberta-base-True,0.4655212431837393,0.42722000842304947,0.4207780847550044,0.37973468944615496,0.3773969190703207,0.37160226570305227,0.3542659545285239,0.37857217765033263,0.3085254593349423,0.33374557053651493,0.33880718143895056,0.3832712996251549,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_albert-xxlarge-v2-True,0.37861193062571785,0.5212015656547714,0.4899624434263354,0.5046568379746696,0.459324624377227,0.4525939578386269,0.4332358259226968,0.41789632425711165,0.4123967866486501,0.42344440645779774,0.4560806606234955,0.4913927638394655,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_allenai-longformer-scico-True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_microsoft-deberta-large-True,0.4255651287466521,0.39618774855712185,0.40711133393067894,0.4180058279060614,0.40984151508971384,0.38416703721460743,0.39075650302596443,0.37930109321684047,0.37119750288958026,0.37120215639945975,0.38448934887468156,0.36318615230249723,0.3887451605616394,0.37463463835721755,0.3864693403431976,0.37960004232449956,0.36020815331507045,0.37917999303410027,0.3655670155557212,0.3401288257462325,0.380216538199505,0.3979332619313039,0.38945365866927795,0.3827059104596707,,,,,,,,,,,,,,,,,,,,,,,,
study_1_openai-gpt-True,0.38071485931662247,0.36359057956836005,0.3649292672467187,0.3538925806381247,0.36097508340849926,0.35431795353803325,0.3378560805872038,0.3916102689795519,0.35174538466663313,0.3763522938480318,0.3484373240690497,0.34570788559764937,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_albert-base-v1-True,0.450396882002273,0.5002672779434002,0.5016760397015164,0.5261655106170069,0.518370476854868,0.5236595337950958,0.5227511489506496,0.5326816220082219,0.5407518226300093,0.5434921747709239,0.5462200672938677,0.5358315540195208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_distilbert-base-uncased-True,0.5756028712840455,0.4065143166627685,0.5387165815798758,0.4765440014615337,0.5495090660460336,0.40309621482226027,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_distilbert-base-multilingual-cased-True,0.4522220611784919,0.3578613090588589,0.5251322490980527,0.49738280942476454,0.5169587202303985,0.52556206866483,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_xlm-roberta-base-True,0.3338543154956472,0.30028308840289597,0.3839136271193826,0.37719261549423905,0.41358888370474,0.4503288719961761,0.41577219094739015,0.43352367713703516,0.46595177908221763,0.4434321026215743,0.497816202134935,0.27261866227204035,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_gpt2-large-True,-0.049986157679411856,0.025254544895259776,0.04727921498661909,0.07976252730013228,0.07502537758774494,0.10326371158756037,0.0941585407118029,0.1574025654434157,0.10277154018887079,0.2051431911163063,0.08406902703866623,0.1561106155218555,0.20748100526008184,0.14915869451536498,0.22236919007044204,0.14497523762650344,0.14325263773108987,0.1238733889076871,0.15961733673751888,0.20052908425359134,0.1553108369989849,0.2344273893383371,0.165215786397613,0.2633424590113507,0.21344858346919318,0.13857700944353873,0.12430403888154051,0.19123934910332524,0.19043957058045466,0.2096342551293488,0.241194746070319,0.20526623396597868,0.2194776831031407,0.21363314774370176,0.14023808791411613,0.07650189178381372,,,,,,,,,,,,
study_1_albert-large-v2-True,0.4575181818640676,0.44854703194488316,0.4551972013456027,0.4096666112096956,0.4256193085958525,0.4255031238513516,0.4004062701778359,0.40444518346491626,0.4193419569592855,0.4143692212121047,0.40714599533495754,0.40976027115042735,0.4329961453075164,0.43133854351287954,0.43166107893186734,0.4456999958468752,0.4406597321940348,0.4477166553339382,0.4507211619917829,0.4750520457249085,0.46795337317409724,0.4630078208305182,0.48302297529254007,0.42785508516656057,,,,,,,,,,,,,,,,,,,,,,,,
study_1_roberta-large-True,0.44357516364841026,0.3641718733104589,0.39927774136045485,0.4506387805117168,0.4005466570274994,0.37635828465752036,0.3635135842976235,0.3391148347886017,0.3375754583068213,0.349128765449619,0.34634989688204576,0.3677365896408216,0.35445322021806686,0.32487114701820413,0.3183176242881708,0.3189701279448307,0.32338531128877523,0.33935536359236723,0.3262872767404414,0.3539827988535498,0.36469838494796364,0.3646625442405814,0.32038011031663594,0.3719183366476596,,,,,,,,,,,,,,,,,,,,,,,,
study_1_albert-base-v2-True,0.46086684054536065,0.5362045746067361,0.5081107362596244,0.49898885520383873,0.4901776572008331,0.47727934536292577,0.4859327453186763,0.4946602312069452,0.5012850886303062,0.5274072691313346,0.5604096121078074,0.51578085755495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
study_1_allenai-longformer-large-4096-finetuned-triviaqa-True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
